{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ac9e354",
   "metadata": {},
   "source": [
    "# A simple CNN model to classify jpeg thumbnails as cloudy (0) or cloudfree (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c117a5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from CNN import CNN\n",
    "from DataGenerator import DataGenerator\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67de06aa",
   "metadata": {},
   "source": [
    "# prepare train, val, test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91f2a106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parcels 39\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"../data/sentinel2_clouds/labels.csv\")\n",
    "# reconstruct full path to jpegs files\n",
    "dataset[\"filename\"] = dataset.apply(lambda x: os.path.join(\"../data/sentinel2_clouds\", x[\"parcel_id\"], x[\"filename\"]+\".jpeg\"), axis=1)\n",
    "# smallest number of samples for a given label (0 or 1) and a given parcel\n",
    "min_samples = dataset.groupby([\"parcel_id\", \"label\"]).agg(\"count\").min()[0]\n",
    "\n",
    "# randomly sample min_samples from each group\n",
    "subset = dataset.groupby([\"parcel_id\", \"label\"]).sample(n=min_samples, replace=False, random_state=seed)\n",
    "parcel_ids = subset[\"parcel_id\"].unique()\n",
    "n_parcels = len(parcel_ids)\n",
    "print(f\"Number of parcels {n_parcels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6df55c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_parcels, test_parcels = train_test_split(parcel_ids, test_size=0.3, random_state=seed, shuffle=True)\n",
    "test_parcels, val_parcels = train_test_split(test_parcels, test_size=0.3, random_state=seed, shuffle=True)\n",
    "\n",
    "train_dataset = pd.concat([dataset[dataset[\"parcel_id\"]==parcel_id] for parcel_id in train_parcels])\n",
    "test_dataset = pd.concat([dataset[dataset[\"parcel_id\"]==parcel_id] for parcel_id in test_parcels])\n",
    "val_dataset = pd.concat([dataset[dataset[\"parcel_id\"]==parcel_id] for parcel_id in val_parcels])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281785a1",
   "metadata": {},
   "source": [
    "**make TF data generators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24e1fd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-07 21:13:36.172038: W tensorflow/stream_executor/platform/default/dso_loader.cc:65] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2021-11-07 21:13:36.172079: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-11-07 21:13:36.172103: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (localhost): /proc/driver/nvidia/version does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    }
   ],
   "source": [
    "DG = DataGenerator()\n",
    "\n",
    "train_ds = DG.get_dataset(\n",
    "    filenames=train_dataset[\"filename\"].to_numpy(), \n",
    "    labels=train_dataset[\"label\"].to_numpy(), \n",
    "    batch_size=5, \n",
    "    n_prefetch=1,\n",
    "    training=True\n",
    ")\n",
    "\n",
    "val_ds = DG.get_dataset(\n",
    "    filenames=val_dataset[\"filename\"].to_numpy(), \n",
    "    labels=val_dataset[\"label\"].to_numpy())\n",
    "\n",
    "test_ds = DG.get_dataset(\n",
    "    filenames=test_dataset[\"filename\"].to_numpy(), \n",
    "    labels=test_dataset[\"label\"].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00485a3e",
   "metadata": {},
   "source": [
    "# make CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25349484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1840/1840 [==============================] - 24s 13ms/step - loss: 0.2916 - accuracy: 0.8871 - val_loss: 0.2742 - val_accuracy: 0.9038\n",
      "Epoch 2/50\n",
      "1840/1840 [==============================] - 24s 13ms/step - loss: 0.2376 - accuracy: 0.9170 - val_loss: 0.2296 - val_accuracy: 0.9143\n",
      "Epoch 3/50\n",
      "1840/1840 [==============================] - 22s 12ms/step - loss: 0.2351 - accuracy: 0.9178 - val_loss: 0.2451 - val_accuracy: 0.9064\n",
      "Epoch 4/50\n",
      "1840/1840 [==============================] - 20s 11ms/step - loss: 0.2223 - accuracy: 0.9227 - val_loss: 0.2450 - val_accuracy: 0.9169\n",
      "Epoch 5/50\n",
      "1840/1840 [==============================] - 20s 11ms/step - loss: 0.2231 - accuracy: 0.9230 - val_loss: 0.2501 - val_accuracy: 0.9169\n",
      "Epoch 6/50\n",
      "1840/1840 [==============================] - 20s 11ms/step - loss: 0.2292 - accuracy: 0.9204 - val_loss: 0.2169 - val_accuracy: 0.9160\n",
      "Epoch 7/50\n",
      "1840/1840 [==============================] - 19s 11ms/step - loss: 0.2187 - accuracy: 0.9238 - val_loss: 0.1958 - val_accuracy: 0.9178\n",
      "Epoch 8/50\n",
      "1840/1840 [==============================] - 20s 11ms/step - loss: 0.2191 - accuracy: 0.9259 - val_loss: 0.2210 - val_accuracy: 0.9204\n",
      "Epoch 9/50\n",
      "1840/1840 [==============================] - 19s 10ms/step - loss: 0.2122 - accuracy: 0.9263 - val_loss: 0.2134 - val_accuracy: 0.9291\n",
      "Epoch 10/50\n",
      "1840/1840 [==============================] - 19s 10ms/step - loss: 0.1975 - accuracy: 0.9305 - val_loss: 0.2055 - val_accuracy: 0.9239\n",
      "Epoch 11/50\n",
      "1840/1840 [==============================] - 20s 11ms/step - loss: 0.1933 - accuracy: 0.9343 - val_loss: 0.2963 - val_accuracy: 0.9046\n",
      "Epoch 12/50\n",
      "1840/1840 [==============================] - 19s 10ms/step - loss: 0.1787 - accuracy: 0.9354 - val_loss: 0.1966 - val_accuracy: 0.9256\n",
      "Epoch 13/50\n",
      "1840/1840 [==============================] - 20s 11ms/step - loss: 0.1752 - accuracy: 0.9379 - val_loss: 0.1855 - val_accuracy: 0.9256\n",
      "Epoch 14/50\n",
      "1840/1840 [==============================] - 19s 10ms/step - loss: 0.1719 - accuracy: 0.9396 - val_loss: 0.2176 - val_accuracy: 0.9151\n",
      "Epoch 15/50\n",
      "1840/1840 [==============================] - 19s 10ms/step - loss: 0.1708 - accuracy: 0.9398 - val_loss: 0.2530 - val_accuracy: 0.9073\n",
      "Epoch 16/50\n",
      "1840/1840 [==============================] - 19s 10ms/step - loss: 0.1631 - accuracy: 0.9441 - val_loss: 0.2612 - val_accuracy: 0.9020\n",
      "Epoch 17/50\n",
      "1840/1840 [==============================] - 20s 11ms/step - loss: 0.1671 - accuracy: 0.9434 - val_loss: 0.1775 - val_accuracy: 0.9291\n",
      "Epoch 18/50\n",
      "1840/1840 [==============================] - 19s 10ms/step - loss: 0.1654 - accuracy: 0.9417 - val_loss: 0.1964 - val_accuracy: 0.9230\n",
      "Epoch 19/50\n",
      "1840/1840 [==============================] - 18s 10ms/step - loss: 0.1620 - accuracy: 0.9438 - val_loss: 0.1943 - val_accuracy: 0.9283\n",
      "Epoch 20/50\n",
      "1840/1840 [==============================] - 21s 11ms/step - loss: 0.1640 - accuracy: 0.9443 - val_loss: 0.1908 - val_accuracy: 0.9248\n",
      "Epoch 21/50\n",
      "1840/1840 [==============================] - 19s 10ms/step - loss: 0.1636 - accuracy: 0.9455 - val_loss: 0.2173 - val_accuracy: 0.9204\n",
      "Epoch 22/50\n",
      "1840/1840 [==============================] - 21s 11ms/step - loss: 0.1620 - accuracy: 0.9436 - val_loss: 0.2029 - val_accuracy: 0.9256\n",
      "Epoch 23/50\n",
      "1840/1840 [==============================] - 25s 13ms/step - loss: 0.1542 - accuracy: 0.9486 - val_loss: 0.1861 - val_accuracy: 0.9440\n",
      "Epoch 24/50\n",
      "1840/1840 [==============================] - 19s 10ms/step - loss: 0.1581 - accuracy: 0.9467 - val_loss: 0.1353 - val_accuracy: 0.9571\n",
      "Epoch 25/50\n",
      "1840/1840 [==============================] - 18s 10ms/step - loss: 0.1585 - accuracy: 0.9482 - val_loss: 0.1901 - val_accuracy: 0.9405\n",
      "Epoch 26/50\n",
      "1840/1840 [==============================] - 19s 10ms/step - loss: 0.1543 - accuracy: 0.9473 - val_loss: 0.1687 - val_accuracy: 0.9423\n",
      "Epoch 27/50\n",
      "1840/1840 [==============================] - 20s 11ms/step - loss: 0.1559 - accuracy: 0.9465 - val_loss: 0.1830 - val_accuracy: 0.9265\n",
      "Epoch 28/50\n",
      "1840/1840 [==============================] - 19s 10ms/step - loss: 0.1565 - accuracy: 0.9482 - val_loss: 0.1758 - val_accuracy: 0.9379\n",
      "Epoch 29/50\n",
      "1840/1840 [==============================] - 19s 10ms/step - loss: 0.1505 - accuracy: 0.9496 - val_loss: 0.1488 - val_accuracy: 0.9484\n",
      "Epoch 30/50\n",
      "1840/1840 [==============================] - 20s 11ms/step - loss: 0.1509 - accuracy: 0.9518 - val_loss: 0.1695 - val_accuracy: 0.9484\n",
      "Epoch 31/50\n",
      "1840/1840 [==============================] - 20s 11ms/step - loss: 0.1521 - accuracy: 0.9489 - val_loss: 0.2423 - val_accuracy: 0.9309\n",
      "Epoch 32/50\n",
      "1840/1840 [==============================] - 19s 10ms/step - loss: 0.1527 - accuracy: 0.9500 - val_loss: 0.1471 - val_accuracy: 0.9659\n",
      "Epoch 33/50\n",
      "1840/1840 [==============================] - 19s 11ms/step - loss: 0.1538 - accuracy: 0.9469 - val_loss: 0.1436 - val_accuracy: 0.9519\n",
      "Epoch 34/50\n",
      "1840/1840 [==============================] - 21s 11ms/step - loss: 0.1521 - accuracy: 0.9481 - val_loss: 0.1431 - val_accuracy: 0.9589\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2b70d8c0a0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)\n",
    "cnn.model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# callbacks\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"./model_weights/model.h5\", save_best_only=True)\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "cnn.model.fit(train_ds, validation_data=val_ds , epochs=50, callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f515802",
   "metadata": {},
   "source": [
    "# reload model from saved weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2b7ea3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-07 21:13:41.787065: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "697/697 [==============================] - 3s 3ms/step - loss: 0.1647 - accuracy: 0.9417\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1647462546825409, 0.9417336583137512]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_t = CNN()\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)\n",
    "cnn_t.model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "cnn_t.model.load_weights(\"./model_weights/model.h5\")\n",
    "# evaluate on test dataset\n",
    "cnn_t.model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cf68f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-07 21:04:06.729271: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "p_false = []\n",
    "for batch in test_ds:\n",
    "    pred = cnn_t.model.predict(batch[0]).flatten()\n",
    "    pred_labels = pred>=0.5\n",
    "    true_labels = batch[1].numpy()\n",
    "    select_false = ((pred_labels==1) & (true_labels==0)) |\\\n",
    "    ((pred_labels==0) & (true_labels==1))\n",
    "    if any(select_false):\n",
    "        p_false.extend(pred[select_false].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9cc72f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# output proba for false classifications are not necessarily close to 0.5\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(p_false)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e23db5e",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cb23f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(path, extensions=(\".tif\", \".tiff\")):\n",
    "    results = []\n",
    "    for r, d, files in os.walk(path):\n",
    "        for f in files:\n",
    "            if f.endswith(extensions):\n",
    "                results.append(os.path.join(r, f))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "309512a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "jpegs = get_files(\"../data/sentinel2_clouds\", extensions=(\"jpeg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5424a0e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46659"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep only unlabeled jpegs\n",
    "jpegs = [jpeg for jpeg in jpegs if os.path.dirname(jpeg).split(\"/\")[-1] not in parcel_ids]\n",
    "len(jpegs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8eaaedd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DG = DataGenerator()\n",
    "\n",
    "inference_ds = DG.get_dataset_inference(\n",
    "    filenames=jpegs, \n",
    "    batch_size=5, \n",
    "    n_prefetch=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ff205d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = cnn_t.model.predict(inference_ds).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21154d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy jpegs into cloud and nocloud folders based on prediction\n",
    "nocloud_folder = \"../data/sentinel2_clouds/inference/nocloud\"\n",
    "cloud_folder = \"../data/sentinel2_clouds/inference/cloud\"\n",
    "os.makedirs(nocloud_folder, exist_ok=True)\n",
    "os.makedirs(cloud_folder, exist_ok=True)\n",
    "for p, jpeg in zip(pred, jpegs):\n",
    "    if p>=0.5: # no cloud\n",
    "        shutil.copyfile(jpeg, os.path.join(nocloud_folder, os.path.basename(jpeg)))\n",
    "    else:\n",
    "        shutil.copyfile(jpeg, os.path.join(cloud_folder, os.path.basename(jpeg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64187f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
